# OFA

> This is the official repo for the paper: UNIFYING ARCHITECTURES, TASKS, AND MODALITIES THROUGH A SIMPLE SEQUENCE-TO-SEQUENCE LEARNING FRAMEWORK

![Overview](examples/overview.png)

OFA is a unified multimodal pretrained model that unifies modalities (i.e., cross-modality, vision, language) and tasks 
(e.g., image generation, visual grounding, image captioning, image classification, text generation, etc.) 
to a simple sequence-tosequence learning framework.

# Examples
## Image Generation (normal query)
![t2i_normal](examples/normal_images.png)

## Image Generation (counterfactual query)
![t2i_counterfactual](examples/counterfactual_images.png)

## Open-Ended VQA
![open_vqa](examples/open_vqa.png)

## grounded QA
![grounded_qa](examples/grounded_qa.png)

## Viusal Grounding
![vg](examples/viusal_grounding.png)